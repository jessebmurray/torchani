{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import os\n",
    "import math\n",
    "import torch.utils.tensorboard\n",
    "import tqdm\n",
    "import pickle\n",
    "import nci\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchani\n",
    "import numpy as np\n",
    "from torchani.aev import AEVComputer\n",
    "# helper function to convert energy unit from Hartree to kcal/mol\n",
    "from torchani.units import hartree2kcalmol\n",
    "\n",
    "# device to run the training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '../NCIA_HB375x10'\n",
    "elements = nci.dataset_elements[dataset]\n",
    "ani1x_values = {'radial_cutoff': 5.2,\n",
    "                'angular_cutoff': 3.5,\n",
    "                'radial_eta': 16.0,\n",
    "                'angular_eta': 8.0,\n",
    "                'radial_dist_divisions': 16,\n",
    "                'angular_dist_divisions': 4,\n",
    "                'zeta': 32.0,\n",
    "                'angle_sections': 8,\n",
    "                'num_species': len(elements)}\n",
    "\n",
    "constants = nci.get_constants(**ani1x_values)\n",
    "data = nci.load_data(dataset)\n",
    "random_state = 0\n",
    "train_size = 0.8\n",
    "batch_size = 10\n",
    "training, validation = train_test_split(data, train_size=train_size, random_state=random_state)\n",
    "trainloader, validloader = nci.get_data_loaders(training, validation, batch_size=batch_size)\n",
    "aev_computer = AEVComputer(**constants)\n",
    "###############################################################################\n",
    "# When iterating the dataset, we will get a dict of name->property mapping\n",
    "#\n",
    "################################# a ##############################################\n",
    "# Now let's define atomic neural networks.\n",
    "aev_dim = aev_computer.aev_length\n",
    "\n",
    "H_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 160),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(160, 128),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(128, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1),\n",
    ")\n",
    "\n",
    "C_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 144),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(144, 112),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(112, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "N_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 128),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(128, 112),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(112, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "O_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 128),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(128, 112),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(112, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "nn = torchani.ANIModel([H_network, C_network, N_network, O_network])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANIModel(\n",
       "  (0): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=160, bias=True)\n",
       "    (1): CELU(alpha=0.1)\n",
       "    (2): Linear(in_features=160, out_features=128, bias=True)\n",
       "    (3): CELU(alpha=0.1)\n",
       "    (4): Linear(in_features=128, out_features=96, bias=True)\n",
       "    (5): CELU(alpha=0.1)\n",
       "    (6): Linear(in_features=96, out_features=1, bias=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=144, bias=True)\n",
       "    (1): CELU(alpha=0.1)\n",
       "    (2): Linear(in_features=144, out_features=112, bias=True)\n",
       "    (3): CELU(alpha=0.1)\n",
       "    (4): Linear(in_features=112, out_features=96, bias=True)\n",
       "    (5): CELU(alpha=0.1)\n",
       "    (6): Linear(in_features=96, out_features=1, bias=True)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=128, bias=True)\n",
       "    (1): CELU(alpha=0.1)\n",
       "    (2): Linear(in_features=128, out_features=112, bias=True)\n",
       "    (3): CELU(alpha=0.1)\n",
       "    (4): Linear(in_features=112, out_features=96, bias=True)\n",
       "    (5): CELU(alpha=0.1)\n",
       "    (6): Linear(in_features=96, out_features=1, bias=True)\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=128, bias=True)\n",
       "    (1): CELU(alpha=0.1)\n",
       "    (2): Linear(in_features=128, out_features=112, bias=True)\n",
       "    (3): CELU(alpha=0.1)\n",
       "    (4): Linear(in_features=112, out_features=96, bias=True)\n",
       "    (5): CELU(alpha=0.1)\n",
       "    (6): Linear(in_features=96, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the weights and biases.\n",
    "#   Pytorch default initialization for the weights and biases in linear layers\n",
    "#   is Kaiming uniform. See: `TORCH.NN.MODULES.LINEAR`_\n",
    "#   We initialize the weights similarly but from the normal distribution.\n",
    "#   The biases were initialized to zero.\n",
    "\n",
    "def init_params(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, a=1.0)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "nn.apply(init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now create a pipeline of AEV Computer --> Neural Networks.\n",
    "model = torchani.nn.Sequential(aev_computer, nn).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(trainloader))\n",
    "# ids, labels, species_coordinates, index_diff = batch\n",
    "\n",
    "# # model(species_coordinates, index_diff)\n",
    "# species, coordinates = species_coordinates\n",
    "# energies = np.array([entry['energies'] for entry in data])\n",
    "# energies\n",
    "# energies.var()\n",
    "# species, aev = aev_computer.forward(species_coordinates, index_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Now let's setup the optimizers. NeuroChem uses Adam with decoupled weight decay\n",
    "# to updates the weights and Stochastic Gradient Descent (SGD) to update the biases.\n",
    "# Moreover, we need to specify different weight decay rate for different layes.\n",
    "#\n",
    "# .. note::\n",
    "#\n",
    "#   The weight decay in `inputtrain.ipt`_ is named \"l2\", but it is actually not\n",
    "#   L2 regularization. The confusion between L2 and weight decay is a common\n",
    "#   mistake in deep learning.  See: `Decoupled Weight Decay Regularization`_\n",
    "#   Also note that the weight decay only applies to weight in the training\n",
    "#   of ANI models, not bias.\n",
    "#\n",
    "# .. _Decoupled Weight Decay Regularization:\n",
    "#   https://arxiv.org/abs/1711.05101\n",
    "\n",
    "\n",
    "AdamW = torch.optim.AdamW([\n",
    "    # H networks\n",
    "    {'params': [H_network[0].weight]},\n",
    "    {'params': [H_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [H_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [H_network[6].weight]},\n",
    "    # C networks\n",
    "    {'params': [C_network[0].weight]},\n",
    "    {'params': [C_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [C_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [C_network[6].weight]},\n",
    "    # N networks\n",
    "    {'params': [N_network[0].weight]},\n",
    "    {'params': [N_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [N_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [N_network[6].weight]},\n",
    "    # O networks\n",
    "    {'params': [O_network[0].weight]},\n",
    "    {'params': [O_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [O_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [O_network[6].weight]},\n",
    "])\n",
    "\n",
    "SGD = torch.optim.SGD([\n",
    "    # H networks\n",
    "    {'params': [H_network[0].bias]},\n",
    "    {'params': [H_network[2].bias]},\n",
    "    {'params': [H_network[4].bias]},\n",
    "    {'params': [H_network[6].bias]},\n",
    "    # C networks\n",
    "    {'params': [C_network[0].bias]},\n",
    "    {'params': [C_network[2].bias]},\n",
    "    {'params': [C_network[4].bias]},\n",
    "    {'params': [C_network[6].bias]},\n",
    "    # N networks\n",
    "    {'params': [N_network[0].bias]},\n",
    "    {'params': [N_network[2].bias]},\n",
    "    {'params': [N_network[4].bias]},\n",
    "    {'params': [N_network[6].bias]},\n",
    "    # O networks\n",
    "    {'params': [O_network[0].bias]},\n",
    "    {'params': [O_network[2].bias]},\n",
    "    {'params': [O_network[4].bias]},\n",
    "    {'params': [O_network[6].bias]},\n",
    "], lr=1e-3)\n",
    "\n",
    "###############################################################################\n",
    "# Setting up a learning rate scheduler to do learning rate decay\n",
    "AdamW_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(AdamW, factor=0.5, patience=100, threshold=0)\n",
    "SGD_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(SGD, factor=0.5, patience=100, threshold=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Collection, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    loss_function,\n",
    "    trainloader,\n",
    "    testloader,\n",
    "    epochs: int = 15,\n",
    "    savepath: Optional[str] = None,\n",
    "    idx: Optional[int] = None,\n",
    "    device=None,\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Model\n",
    "    optimizer\n",
    "        Optimizer\n",
    "    loss_function\n",
    "        Loss function\n",
    "    AEVC: torchani.AEVComputer\n",
    "        AEVComputer\n",
    "    trainloader:\n",
    "        Train set loader\n",
    "    testloader:\n",
    "        Test (validation) set loader\n",
    "    epochs: int\n",
    "        Number of training epochs\n",
    "    savepath:\n",
    "        Save path for best performing model\n",
    "    idx: int\n",
    "        Inded (for multiple trainings)\n",
    "    device:\n",
    "        Computation device\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[float], List[float]]\n",
    "        Train loss and validation loss\n",
    "    \"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Move model and AEVComputer to device\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses: List[float] = []\n",
    "    valid_losses: List[float] = []\n",
    "\n",
    "    best_valid_loss = np.inf\n",
    "\n",
    "    for epoch in tqdm.trange(epochs, desc=\"Training\"):\n",
    "\n",
    "        # Model in training mode\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss: float = 0.0\n",
    "\n",
    "        # Training\n",
    "        for _, labels, species_coordinates, index_diff in trainloader:\n",
    "\n",
    "            # Move data to device\n",
    "            labels = labels.to(device)\n",
    "            species = species_coordinates[0].to(device)\n",
    "            coordinates = species_coordinates[1].to(device)\n",
    "            index_diff = index_diff.to(device)\n",
    "\n",
    "\n",
    "            _, predicted_energies = model((species, coordinates), index_diff)\n",
    "            predicted_energies = predicted_energies.to(torch.float64)\n",
    "\n",
    "            loss = loss_function(predicted_energies, labels)\n",
    "\n",
    "\n",
    "            # Update weights\n",
    "            AdamW.zero_grad()\n",
    "            SGD.zero_grad()\n",
    "            loss.backward()\n",
    "            AdamW.step()\n",
    "            SGD.step()\n",
    "\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        else:\n",
    "            valid_loss: float = 0.0\n",
    "\n",
    "            # Model in evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            # Validation\n",
    "            with torch.no_grad():\n",
    "                for _, labels, species_coordinates, index_diff in testloader:\n",
    "\n",
    "                    # Move data to device\n",
    "                    labels = labels.to(device)\n",
    "                    species = species_coordinates[0].to(device)\n",
    "                    coordinates = species_coordinates[1].to(device)\n",
    "                    index_diff = index_diff.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    _, predicted_energies = model((species, coordinates), index_diff)\n",
    "                    predicted_energies = predicted_energies.to(torch.float64)\n",
    "\n",
    "                    valid_loss += loss_function(predicted_energies, labels).item()\n",
    "\n",
    "            # Normalise losses\n",
    "            epoch_loss /= len(trainloader)\n",
    "            valid_loss /= len(testloader)\n",
    "\n",
    "            # Save best model\n",
    "            if valid_loss < best_valid_loss and savepath is not None:\n",
    "                # TODO: Save Optimiser\n",
    "                if idx is None:\n",
    "                    modelname = os.path.join(savepath, \"best.pth\")\n",
    "                else:\n",
    "                    modelname = os.path.join(savepath, f\"best_{idx}.pth\")\n",
    "\n",
    "                best_epoch = epoch\n",
    "                best_valid_loss = valid_loss\n",
    "\n",
    "            train_losses.append(epoch_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "            print(epoch_loss, valid_loss)\n",
    "\n",
    "\n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▎         | 1/40 [00:04<03:11,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 2/40 [00:09<03:04,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 3/40 [00:14<02:58,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 3/40 [00:16<03:22,  5.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jessemurray/Desktop/Main/Rotation1/Code/torchani/train.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jessemurray/Desktop/Main/Rotation1/Code/torchani/train.ipynb#ch0000076?line=1'>2</a>\u001b[0m mse \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jessemurray/Desktop/Main/Rotation1/Code/torchani/train.ipynb#ch0000076?line=2'>3</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m40\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jessemurray/Desktop/Main/Rotation1/Code/torchani/train.ipynb#ch0000076?line=3'>4</a>\u001b[0m train_losses, valid_losses \u001b[39m=\u001b[39m train(model, mse, trainloader, validloader, epochs\u001b[39m=\u001b[39;49mepochs, savepath\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/jessemurray/Desktop/Main/Rotation1/Code/torchani/train.ipynb Cell 7'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_function, trainloader, testloader, epochs, savepath, idx, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jessemurray/Desktop/Main/Rotation1/Code/torchani/train.ipynb#ch0000075?line=77'>78</a>\u001b[0m AdamW\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jessemurray/Desktop/Main/Rotation1/Code/torchani/train.ipynb#ch0000075?line=78'>79</a>\u001b[0m SGD\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jessemurray/Desktop/Main/Rotation1/Code/torchani/train.ipynb#ch0000075?line=79'>80</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jessemurray/Desktop/Main/Rotation1/Code/torchani/train.ipynb#ch0000075?line=80'>81</a>\u001b[0m AdamW\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jessemurray/Desktop/Main/Rotation1/Code/torchani/train.ipynb#ch0000075?line=81'>82</a>\u001b[0m SGD\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "mse = torch.nn.MSELoss()\n",
    "epochs = 40\n",
    "train_losses, valid_losses = train(model, mse, trainloader, validloader, epochs=epochs, savepath='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.345996567549538,\n",
       " 10.135153795757192,\n",
       " 10.017717671913946,\n",
       " 10.0829897331543,\n",
       " 9.934686945138225,\n",
       " 10.123971644934258,\n",
       " 9.830558752681853,\n",
       " 9.905816489287313,\n",
       " 9.880975896356455,\n",
       " 9.728642803488265,\n",
       " 9.880769683113169,\n",
       " 9.862523204394945,\n",
       " 10.153111565354525,\n",
       " 9.99571707568444,\n",
       " 9.940830473949921]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.645850110657591,\n",
       " 9.93084139366282,\n",
       " 9.853266364283586,\n",
       " 9.81873092948209,\n",
       " 9.726423168743883,\n",
       " 9.724835540207359,\n",
       " 9.682365661474405,\n",
       " 9.671551871652099,\n",
       " 9.61286624174033,\n",
       " 9.649055442748033,\n",
       " 9.630369121647812,\n",
       " 9.604778037375649,\n",
       " 9.576343490779585,\n",
       " 9.57597172225012,\n",
       " 9.56329573008717]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Train the model by minimizing the MSE loss, until validation RMSE no longer\n",
    "# improves during a certain number of steps, decay the learning rate and repeat\n",
    "# the same process, stop until the learning rate is smaller than a threshold.\n",
    "#\n",
    "# We first read the checkpoint files to restart training. We use `latest.pt`\n",
    "# to store current training state.\n",
    "latest_checkpoint = 'latest.pt'\n",
    "\n",
    "###############################################################################\n",
    "# Resume training from previously saved checkpoints:\n",
    "if os.path.isfile(latest_checkpoint):\n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    nn.load_state_dict(checkpoint['nn'])\n",
    "    AdamW.load_state_dict(checkpoint['AdamW'])\n",
    "    SGD.load_state_dict(checkpoint['SGD'])\n",
    "    AdamW_scheduler.load_state_dict(checkpoint['AdamW_scheduler'])\n",
    "    SGD_scheduler.load_state_dict(checkpoint['SGD_scheduler'])\n",
    "\n",
    "###############################################################################\n",
    "# During training, we need to validate on validation set and if validation error\n",
    "# is better than the best, then save the new best model to a checkpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    # run validation\n",
    "    mse_sum = torch.nn.MSELoss(reduction='sum')\n",
    "    total_mse = 0.0\n",
    "    count = 0\n",
    "    model.train(False)\n",
    "    with torch.no_grad():\n",
    "        for properties in validation:\n",
    "            species = properties['species'].to(device)\n",
    "            coordinates = properties['coordinates'].to(device).float()\n",
    "            true_energies = properties['energies'].to(device).float()\n",
    "            _, predicted_energies = model((species, coordinates))\n",
    "            total_mse += mse_sum(predicted_energies, true_energies).item()\n",
    "            count += predicted_energies.shape[0]\n",
    "    model.train(True)\n",
    "    return hartree2kcalmol(math.sqrt(total_mse / count))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# We will also use TensorBoard to visualize our training process\n",
    "tensorboard = torch.utils.tensorboard.SummaryWriter()\n",
    "\n",
    "###############################################################################\n",
    "# Finally, we come to the training loop.\n",
    "#\n",
    "# In this tutorial, we are setting the maximum epoch to a very small number,\n",
    "# only to make this demo terminate fast. For serious training, this should be\n",
    "# set to a much larger value\n",
    "mse = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "print(\"training starting from epoch\", AdamW_scheduler.last_epoch + 1)\n",
    "max_epochs = 10\n",
    "early_stopping_learning_rate = 1.0E-5\n",
    "best_model_checkpoint = 'best.pt'\n",
    "\n",
    "for _ in range(AdamW_scheduler.last_epoch + 1, max_epochs):\n",
    "    rmse = validate()\n",
    "    print('RMSE:', rmse, 'at epoch', AdamW_scheduler.last_epoch + 1)\n",
    "\n",
    "    learning_rate = AdamW.param_groups[0]['lr']\n",
    "\n",
    "    if learning_rate < early_stopping_learning_rate:\n",
    "        break\n",
    "\n",
    "    # checkpoint\n",
    "    if AdamW_scheduler.is_better(rmse, AdamW_scheduler.best):\n",
    "        torch.save(nn.state_dict(), best_model_checkpoint)\n",
    "\n",
    "    AdamW_scheduler.step(rmse)\n",
    "    SGD_scheduler.step(rmse)\n",
    "\n",
    "    tensorboard.add_scalar('validation_rmse', rmse, AdamW_scheduler.last_epoch)\n",
    "    tensorboard.add_scalar('best_validation_rmse', AdamW_scheduler.best, AdamW_scheduler.last_epoch)\n",
    "    tensorboard.add_scalar('learning_rate', learning_rate, AdamW_scheduler.last_epoch)\n",
    "\n",
    "    for i, properties in tqdm.tqdm(\n",
    "        enumerate(training),\n",
    "        total=len(training),\n",
    "        desc=\"epoch {}\".format(AdamW_scheduler.last_epoch)\n",
    "    ):\n",
    "        species = properties['species'].to(device)\n",
    "        coordinates = properties['coordinates'].to(device).float()\n",
    "        true_energies = properties['energies'].to(device).float()\n",
    "        num_atoms = (species >= 0).sum(dim=1, dtype=true_energies.dtype)\n",
    "        _, predicted_energies = model((species, coordinates))\n",
    "\n",
    "        loss = (mse(predicted_energies, true_energies) / num_atoms.sqrt()).mean()\n",
    "\n",
    "        AdamW.zero_grad()\n",
    "        SGD.zero_grad()\n",
    "        loss.backward()\n",
    "        AdamW.step()\n",
    "        SGD.step()\n",
    "\n",
    "        # write current batch loss to TensorBoard\n",
    "        tensorboard.add_scalar('batch_loss', loss, AdamW_scheduler.last_epoch * len(training) + i)\n",
    "\n",
    "    torch.save({\n",
    "        'nn': nn.state_dict(),\n",
    "        'AdamW': AdamW.state_dict(),\n",
    "        'SGD': SGD.state_dict(),\n",
    "        'AdamW_scheduler': AdamW_scheduler.state_dict(),\n",
    "        'SGD_scheduler': SGD_scheduler.state_dict(),\n",
    "    }, latest_checkpoint)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c30f2af5f468e7f5b45bcc30fca5f4886c90d54777aed916ed5f6294dfb24bf2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
